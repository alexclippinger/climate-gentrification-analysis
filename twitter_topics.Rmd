---
title: "EDS231"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

```{r packages, results='hide', message=FALSE, warning=FALSE}
library(quanteda)
# devtools::install_github("quanteda/quanteda.sentiment") 
library(quanteda.sentiment)
library(quanteda.textstats)
library(tidyverse)
# library(tibble)
library(tidytext)
# library(lubridate)
# library(wordcloud)
# library(reshape2)
# library(pals)
library(readxl)
library(pdftools)
library(tm)
library(topicmodels)
library(ldatuning)
```

## Load Tweets

```{r tweet_data}
raw_tweets <- read_excel("data/mentions_1.xlsx", skip = 7)

tweets <- tibble(text = raw_tweets$Title, # Why not full text?
                 id = seq(1:length(raw_tweets$Title)),
                 date = date(raw_tweets$Date))

# Simple plot of tweets per day
tweets %>%
  count(date) %>%
  ggplot(aes(x = date, y = n)) +
  geom_line() +
  theme_light()
```

## Corpus

```{r corpus}
cg_corp <- corpus(x = tweets, text_field = "text")
cg_corp.stats <- summary(cg_corp)
head(cg_corp.stats, n = 25)
toks <- tokens(cg_corp, remove_punct = TRUE, remove_numbers = TRUE)
#I added some project-specific stop words here
add_stops <- c(stopwords("en"), "rt", "n")
toks1 <- tokens_select(toks, pattern = add_stops, selection = "remove")
```

```{r dfm}
dfm_comm<- dfm(toks1, tolower = TRUE)
dfm <- dfm_wordstem(dfm_comm)
# dfm <- dfm_trim(dfm, min_docfreq = 2) #remove terms only appearing in one doc (min_termfreq = 10)

print(head(dfm))

#remove rows (docs) with all zeros
sel_idx <- slam::row_sums(dfm) > 0 
dfm <- dfm[sel_idx, ]
#comments_df <- dfm[sel_idx, ]
```

```{r LDA_modeling}
#
result <- FindTopicsNumber(
  dfm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```

```{r}
k <- 7

topicModel_k7 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = 100))

tmResult <- posterior(topicModel_k7)
terms(topicModel_k7, 10)
theta <- tmResult$topics
beta <- tmResult$terms
vocab <- (colnames(beta))
```


```{r top_terms_topic}
comment_topics <- tidy(topicModel_k7, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```


```{r plot_top_terms}
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

```{r topic_names}
top5termsPerTopic <- terms(topicModel_k7, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
```

We can explore the theta matrix, which contains the distribution of each topic over each document

```{r topic_dists}
exampleIds <- c(1, 2, 3, 4, 5, 6)
N <- length(exampleIds)

#lapply(epa_corp[exampleIds], as.character) #uncomment to view example text
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document=factor(1:N)), variable.name = "topic", id.vars = "document")  
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```

